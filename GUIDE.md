# Hybrid RAG E-Commerce Product Search — System Guide

## Table of Contents

- [System Architecture](#system-architecture)
- [Tech Stack](#tech-stack)
- [Prerequisites](#prerequisites)
- [Step-by-Step Setup](#step-by-step-setup)
- [Running the System](#running-the-system)
- [Testing](#testing)
- [Project Structure](#project-structure)
- [Troubleshooting](#troubleshooting)

---

## System Architecture

### How the Pipeline Works

```
                         USER QUERY
                    "running shoes under $150"
                             |
                             v
                    +-------------------+
                    |   QUERY PARSER    |
                    |                   |
                    |  Regex extracts:  |
                    |  max_price = 150  |
                    |  search_text =    |
                    |  "running shoes"  |
                    |  category_hint =  |
                    |  (keyword match)  |
                    +--------+----------+
                             |
              +--------------+--------------+
              v                             v
    +-------------------+         +-------------------+
    | DENSE RETRIEVAL   |         | SPARSE RETRIEVAL  |
    |                   |         |                   |
    | BGE-small-en-v1.5 |         | BM25Okapi         |
    | 384-dim embeddings|         | Regex tokenizer   |
    | Cosine similarity |         | Keyword matching  |
    | via dot product   |         | Preserves model   |
    | on L2-normalized  |         | numbers & hyphens |
    | vectors (NumPy)   |         | (e.g. USB-C)      |
    |                   |         |                   |
    | Top 30 results    |         | Top 30 results    |
    +---------+---------+         +---------+---------+
              |                             |
              +-------------+---------------+
                            v
                 +----------------------+
                 | RECIPROCAL RANK      |
                 | FUSION (RRF)         |
                 |                      |
                 | score(d) =           |
                 |   1/(60+rank_dense)  |
                 |  +1/(60+rank_sparse) |
                 |                      |
                 | Rank-based, avoids   |
                 | score normalization  |
                 | issues. Products in  |
                 | BOTH lists get the   |
                 | strongest boost.     |
                 |                      |
                 | Top 10 candidates    |
                 +----------+-----------+
                            v
                 +----------------------+
                 | POST-RETRIEVAL       |
                 | FILTERS              |
                 |                      |
                 | Price: <= max_price  |
                 |        >= min_price  |
                 | (Applied AFTER       |
                 |  fusion so both      |
                 |  retrievers get      |
                 |  symmetric pools)    |
                 |                      |
                 | Top 5 final results  |
                 +----------+-----------+
                            v
                 +----------------------+
                 | GROUNDED RESPONSE    |
                 |                      |
                 | Product cards:       |
                 |   PROGRAMMATIC       |
                 |   (title, price,     |
                 |    URL from catalog) |
                 |                      |
                 | Explanations:        |
                 |   LOCAL LLM          |
                 |   (one sentence per  |
                 |    product, strict   |
                 |    grounding rules)  |
                 +----------+-----------+
                            v
                      FINAL OUTPUT
```

### Why Hybrid Retrieval?

E-commerce queries contain both **semantic intent** and **specific keywords**. Neither retrieval method alone handles both well:

| Query Type | Dense (Embeddings) | Sparse (BM25) |
|---|---|---|
| "comfortable headphones for long flights" | Excellent — captures semantic meaning | Poor — no keyword overlap with "noise-canceling over-ear" |
| "Sony WH-1000XM5" | Poor — model numbers aren't semantically meaningful | Excellent — exact keyword match |
| "affordable running shoes for women" | Good — understands intent | Good — matches "running shoes" |

By combining both with RRF, the system gets the best of both worlds.

### Grounding Strategy

To prevent LLM hallucination, the output is split into two layers:

- **Programmatic layer** (guaranteed accurate): Product title, price, URL, and category are pulled directly from the catalog. These fields are **never** generated by the LLM.
- **LLM layer** (constrained): The local model only writes a one-sentence "why it matches" explanation per product, with strict prompt rules to reference only the provided product data.

If the LLM is unavailable (LM Studio not running), the system gracefully falls back to generic explanations while still returning accurate product cards.

---

## Tech Stack

| Component | Technology | Why |
|-----------|-----------|-----|
| Dense Retrieval | `BAAI/bge-small-en-v1.5` + NumPy | High-quality 384-dim embeddings; cosine similarity via dot product on L2-normalized vectors; no external vector DB needed at 5K scale |
| Sparse Retrieval | `rank-bm25` (BM25Okapi) | Standard keyword retrieval; custom regex tokenizer preserves hyphenated terms (USB-C, WH-1000XM5) and model numbers |
| Fusion | Reciprocal Rank Fusion (RRF) | Rank-based merging avoids score normalization issues between cosine similarity (0–1) and BM25 scores (0–30+) |
| LLM | Local model via LM Studio (OpenAI-compatible API) | Free, local, no API keys; `openai` Python SDK makes the code portable to any OpenAI-compatible endpoint |
| Embeddings Storage | NumPy `.npy` files | At ~5,000 products, brute-force dot product is <5ms; avoids ChromaDB/FAISS dependencies |
| Dataset | McAuley-Lab/Amazon-Reviews-2023 (HuggingFace) | Real Amazon ASINs for verifiable product URLs, rich metadata (title, description, features, price) |
| CLI | `typer` + `rich` | Clean terminal interface with progress bars, tables, and formatted markdown output |
| Config | YAML + Python dataclass | All parameters in one file, type-safe defaults |

### Key Dependencies

```
sentence-transformers    # Embedding model (BAAI/bge-small-en-v1.5)
rank-bm25               # BM25Okapi sparse retrieval
numpy                   # Vector storage & cosine similarity
openai                  # LM Studio client (OpenAI-compatible API)
huggingface-hub         # Dataset download
pandas + pyarrow        # Parquet file loading
typer + rich            # CLI and terminal formatting
pyyaml                  # Configuration
pytest                  # Testing
```

---

## Prerequisites

1. **Python 3.10+** installed
2. **LM Studio** installed — download from [https://lmstudio.ai/](https://lmstudio.ai/)
3. **A model loaded in LM Studio** — recommended: `Mistral 7B Instruct v0.3` (Q4_K_M quantization, ~4GB)
4. **~2GB free disk space** for data, embeddings, and the embedding model cache

---

## Step-by-Step Setup

### 1. Clone and Enter the Project

```bash
git clone <your-repo-url>
cd hybrid-rag-ecommerce
```

### 2. Create a Virtual Environment

```bash
# Create
python -m venv venv

# Activate (Windows PowerShell)
venv\Scripts\activate

# Activate (macOS/Linux)
source venv/bin/activate
```

### 3. Install Dependencies

```bash
pip install -r requirements.txt
```

This installs sentence-transformers (which pulls PyTorch), rank-bm25, openai, and all other dependencies. First install may take a few minutes due to PyTorch.

### 4. Download Data and Build Indices

```bash
python scripts/setup_data.py
```

This single command:
1. Downloads ~5,000 products from 4 Amazon categories via HuggingFace (Electronics, Cell Phones & Accessories, Toys & Games, Arts Crafts & Sewing)
2. Filters out products missing title, price, or both description and features
3. Cleans prices, deduplicates by ASIN, samples ~1,250 per category
4. Saves processed data to `data/products.json`
5. Encodes all products with BGE-small-en-v1.5 and saves embeddings to `indices/dense/`
6. Builds BM25 index and saves to `indices/bm25_index.pkl`
7. Creates product lookup table at `indices/product_lookup.json`

**First run takes 3-5 minutes** (downloading data + encoding 5,000 embeddings). Subsequent runs skip the download if `data/products.json` already exists.

### 5. Set Up LM Studio

1. Open **LM Studio**
2. Go to the **Discover** tab and search for `Mistral 7B Instruct v0.3`
3. Download the **Q4_K_M** quantization (~4GB)
4. Go to the **Local Server** tab (left sidebar, server icon)
5. Select the downloaded model
6. Click **Start Server** — it should start on `http://localhost:1234`
7. Verify it's running: you should see "Server started" in LM Studio

**No API keys needed.** The system uses the `openai` Python SDK pointed at `localhost:1234`.

---

## Running the System

### Single Query (with LLM explanations)

```bash
python scripts/search.py "affordable wireless headphones"
```

**Note for PowerShell users:** If your query contains a `$` sign (e.g., `$50`), use **single quotes** to prevent variable interpolation:

```powershell
python scripts/search.py 'wireless headphones under $50'
```

### Retrieval Only (no LLM — faster, useful for debugging)

```bash
python scripts/search.py "Sony noise canceling headphones" --no-llm
```

This skips the LLM call and shows raw retrieval results in a table format. Useful for:
- Testing retrieval quality without LM Studio running
- Faster iteration when debugging
- Comparing results across different queries

### Limit Number of Results

```bash
python scripts/search.py "laptop under $500" --top-k 3
```

### Interactive Mode

```bash
python scripts/search.py --interactive
```

Opens a search prompt where you can type multiple queries. Type `quit` to exit.

### Interactive Mode + No LLM

```bash
python scripts/search.py --interactive --no-llm
```

### Example Queries to Try

These queries test different aspects of the system:

```bash
# Semantic search (dense retrieval shines)
python scripts/search.py 'comfortable headphones for long flights' --no-llm

# Exact keyword search (sparse retrieval shines)
python scripts/search.py 'USB-C charger' --no-llm

# Price filtering
python scripts/search.py 'wireless headphones under $30' --no-llm

# Price range
python scripts/search.py 'phone case between $5 and $15' --no-llm

# Cross-category
python scripts/search.py 'gift for kids under $25' --no-llm

# Brand search
python scripts/search.py 'Samsung phone case' --no-llm

# Full pipeline with LLM (requires LM Studio running)
python scripts/search.py 'noise canceling headphones for office work'
```

---

## Testing

### Run All Tests

```bash
pytest tests/ -v
```

Expected output: **38 tests, all passing.**

### What the Tests Cover

**`tests/test_retrieval.py`** — 22 tests:

| Test Group | What It Covers |
|---|---|
| `TestQueryParser` (12 tests) | Price extraction (`under $100`, `above $200`, `between $300 and $600`), price with commas (`$1,500`), model numbers not parsed as prices (`iPhone 12`), `4K monitor` edge case, category hint detection, empty search text fallback |
| `TestSparseRetriever` (5 tests) | BM25 tokenization basics, hyphenated term preservation (`WH-1000XM5`, `USB-C`), short token removal, search returns results, empty query handling |
| `TestRRF` (5 tests) | Basic fusion, overlap boosting (products in both lists rank higher), single list, empty lists, top_k limiting |

**`tests/test_pipeline.py`** — 16 tests:

| Test Group | What It Covers |
|---|---|
| `TestDataLoader` (12 tests) | Price parsing (basic, commas, ranges, None, empty, invalid, negative), `safe_join` (list, None, string, mixed types), `build_searchable_text` field combination |
| `TestLLMResponseParsing` (4 tests) | Valid response parsing, partial response with fallback defaults, empty response, malformed/nonsense response graceful handling |

### Run Specific Test Files

```bash
# Just retrieval tests
pytest tests/test_retrieval.py -v

# Just pipeline/data loader tests
pytest tests/test_pipeline.py -v
```

### Run a Specific Test

```bash
pytest tests/test_retrieval.py::TestQueryParser::test_price_range -v
```

---

## Project Structure

```
hybrid-rag-ecommerce/
├── config.yaml                 # All configurable parameters
├── requirements.txt            # Python dependencies
├── .gitignore                  # Excludes data/, indices/, cache
├── README.md                   # Project overview
├── GUIDE.md                    # This file
│
├── src/
│   ├── __init__.py
│   ├── config.py               # Config dataclass + YAML loader
│   ├── data_loader.py          # Product schema, HuggingFace download,
│   │                           #   preprocessing, JSON I/O
│   ├── query_parser.py         # Regex price extraction, category hints
│   ├── retrieval.py            # DenseRetriever (BGE + NumPy),
│   │                           #   SparseRetriever (BM25),
│   │                           #   RRF fusion, hybrid_search()
│   ├── llm.py                  # LM Studio client, prompt template,
│   │                           #   grounded response builder, response parser
│   └── pipeline.py             # HybridRAGPipeline orchestration class
│                               #   (parse -> retrieve -> fuse -> filter -> respond)
│
├── scripts/
│   ├── setup_data.py           # One-command data download + index building
│   └── search.py               # CLI: single query, --no-llm, --interactive
│
├── tests/
│   ├── __init__.py
│   ├── test_retrieval.py       # Query parser, BM25 tokenizer, RRF fusion tests
│   └── test_pipeline.py        # Data loader, LLM response parsing tests
│
├── data/
│   └── products.json           # 5,000 products (generated by setup_data.py)
│
└── indices/
    ├── dense/
    │   ├── embeddings.npy      # 5000 x 384 float32 embedding matrix
    │   └── product_ids.json    # Ordered product ID list
    ├── bm25_index.pkl          # Pickled BM25Okapi index
    └── product_lookup.json     # Product ID -> full product data mapping
```

### Source File Descriptions

| File | Lines | Purpose |
|---|---|---|
| `config.py` | ~54 | Loads `config.yaml` into a typed `Config` dataclass. Falls back to sensible defaults if YAML missing. |
| `data_loader.py` | ~233 | `Product` dataclass schema. Downloads from HuggingFace via `hf_hub_download` (parquet first, JSONL fallback). Cleans prices, joins description/features, deduplicates, saves JSON. |
| `query_parser.py` | ~113 | Regex-based extraction of `min_price`, `max_price` from natural language. Keyword-based category hint. Strips price language from search text to avoid polluting embeddings. |
| `retrieval.py` | ~209 | `DenseRetriever`: encode with sentence-transformers, search via `np.dot()`. `SparseRetriever`: BM25 with custom tokenizer preserving hyphens. `reciprocal_rank_fusion()`: rank-based fusion. `hybrid_search()`: orchestrates both + RRF. |
| `llm.py` | ~157 | `LLMClient`: wraps `openai.OpenAI` pointed at LM Studio. Prompt template with strict grounding rules. `build_grounded_response()`: programmatic product cards + LLM explanations. `parse_llm_response()`: extracts per-product explanations with fallback defaults. |
| `pipeline.py` | ~253 | `HybridRAGPipeline`: orchestrates the full flow. `search()`: parse -> hybrid retrieve -> RRF -> filter -> LLM -> grounded output. `search_retrieval_only()`: same without LLM. `index()`: builds all indices from product JSON. |

---

## Troubleshooting

### "ModuleNotFoundError: No module named 'typer'"

Dependencies not installed. Run:
```bash
pip install -r requirements.txt
```

### "Product file not found" or "Dense index not found"

Indices haven't been built. Run:
```bash
python scripts/setup_data.py
```

### LM Studio not connecting / "Using default explanations"

1. Make sure LM Studio is open and the server is running (Local Server tab → Start Server)
2. Check that it's on port 1234: look for `http://localhost:1234` in LM Studio
3. Make sure a model is loaded (select one in the server tab before starting)
4. The system still works without LM Studio — you just get generic explanations instead of AI-generated ones

### PowerShell eats `$` signs in queries

Use single quotes instead of double quotes:
```powershell
# Wrong (PowerShell treats $50 as a variable)
python scripts/search.py "headphones under $50"

# Correct
python scripts/search.py 'headphones under $50'
```

### "No products found matching..." for a valid query

The price filter may be too strict. Try without a price constraint first:
```bash
python scripts/search.py 'your query here' --no-llm
```

If results appear without price filtering, the constraint is filtering everything out — try a wider range.

### Setup downloads only some categories

Some HuggingFace categories only have JSONL files (slower to download). The system tries parquet first, then falls back to JSONL. If a category fails entirely, a warning is printed but the system continues with available categories.
